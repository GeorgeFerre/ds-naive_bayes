{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "    # There is also a BernoulliNB for a dataset with binary predictors\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "SWBAT:\n",
    "\n",
    "- describe how Bayes's Theorem can be used to make predictions of a target;\n",
    "- identify the appropriate variant of Naive Bayes models for a particular business problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Bayes's Theorem for Classification\n",
    "\n",
    "Let's recall Bayes's Theorem:\n",
    "\n",
    "$\\large P(h|e) = \\frac{P(h)P(e|h)}{P(e)}$\n",
    "\n",
    "**Does this look like a classification problem?**\n",
    "\n",
    "- Suppose we have three competing hypotheses $\\{h_1, h_2, h_3\\}$ that would explain our evidence $e$.\n",
    "    - Then we could use Bayes's Theorem to calculate the posterior probabilities for each of these three:\n",
    "        - $P(h_1|e) = \\frac{P(h_1)P(e|h_1)}{P(e)}$\n",
    "        - $P(h_2|e) = \\frac{P(h_2)P(e|h_2)}{P(e)}$\n",
    "        - $P(h_3|e) = \\frac{P(h_3)P(e|h_3)}{P(e)}$\n",
    "        \n",
    "- Suppose the evidence is a collection of elephant weights.\n",
    "\n",
    "- Suppose each of the three hypotheses claims that the elephant whose measurements we have belongs to one of the three extant elephant species (*L. africana*, *L. cyclotis*, and *E. maximus*).\n",
    "\n",
    "In that case the left-hand sides of these equations represent the probability that the elephant in question belongs to a given species.\n",
    "\n",
    "If we think of the species as our target, then **this is just an ordinary classification problem**.\n",
    "\n",
    "What about the right-hand sides of the equations? **These other probabilities we can calculate from our dataset.**\n",
    "\n",
    "- The priors can simply be taken to be the percentages of the different classes in the dataset.\n",
    "- What about the likelihoods?\n",
    "    - If the relevant features are **categorical**, we can simply count the numbers of each category in the dataset. For example, if the features are whether the elephant has tusks or not, then, to calculate the likelihoods, we'll just count the tusked and non-tuksed elephants per species.\n",
    "    - If the relevant features are **numerical**, we'll have to do something else. A good way of proceeding is to rely on (presumed) underlying distributions of the data. [Here](https://medium.com/analytics-vidhya/use-naive-bayes-algorithm-for-categorical-and-numerical-data-classification-935d90ab273f) is an example of using the normal distribution to calculate likelihoods. We'll follow this idea below for our elephant data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elephant Example\n",
    "\n",
    "Suppose we have a dataset that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elephs = pd.read_csv('data/elephants.csv', usecols=['height (in)',\n",
    "                                                   'species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elephs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.kdeplot(data=elephs[elephs['species'] == 'maximus']['height (in)'],\n",
    "            ax=ax, label='maximus')\n",
    "sns.kdeplot(data=elephs[elephs['species'] == 'africana']['height (in)'],\n",
    "            ax=ax, label='africana')\n",
    "sns.kdeplot(data=elephs[elephs['species'] == 'cyclotis']['height (in)'],\n",
    "            ax=ax, label='cyclotis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes by Hand\n",
    "\n",
    "Suppose we want to make prediction of species for some new elephant whose weight we've just recorded. We'll suppose the new elephant has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ht = 263"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to calculate is the mean and standard deviation for height and weight for each elephant species. We'll use these to calculate the relevant likelihoods.\n",
    "\n",
    "So:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_stats = elephs[elephs['species'] == 'maximus'].describe().loc[['mean', 'std'], :]\n",
    "max_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyc_stats = elephs[elephs['species'] == 'cyclotis'].describe().loc[['mean', 'std'], :]\n",
    "cyc_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afr_stats = elephs[elephs['species'] == 'africana'].describe().loc[['mean', 'std'], :]\n",
    "afr_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elephs['species'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of Likelihoods\n",
    "\n",
    "We'll use the PDFs of the normal distributions with the discovered means and stds to calculate likelihoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm(loc=max_stats['height (in)'][0],\n",
    "           scale=max_stats['height (in)'][1]).pdf(263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm(loc=cyc_stats['height (in)'][0],\n",
    "          scale=cyc_stats['height (in)'][1]).pdf(263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm(loc=afr_stats['height (in)'][0],\n",
    "          scale=afr_stats['height (in)'][1]).pdf(263)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posteriors\n",
    "\n",
    "What we have just calculated are the likelihoods, i.e.:\n",
    "\n",
    "- P(weight=7009 | species=maximus) = 2.04%,\n",
    "- P(weight=7009 | species=cyclotis) = 1.50%, and\n",
    "- P(height=263 | species=africana) = 0.90%.\n",
    "\n",
    "(Notice that they do NOT sum to 1!) But what we'd really like to know are the posteriors. I.e. what are:\n",
    "\n",
    "- P(species=maximus | height=263),\n",
    "- P(species=cyclotis | height=263), and\n",
    "- P(species=africana | height=263)?\n",
    "\n",
    "Since we have equal numbers of each species, every prior is equal to $\\frac{1}{3}$. Thus we can calculate the probability of the evidence:\n",
    "\n",
    "P(height=263) = $\\frac{1}{3}(0.0204 + 0.0150 + 0.0090) = 0.0148$,\n",
    "\n",
    "and therefore calculate the posteriors using Bayes's Theorem:\n",
    "\n",
    "- P(species=maximus | height=263) = $\\frac{1}{3}\\frac{0.0204}{0.0148} = 45.9\\%$;\n",
    "- P(species=cyclotis | height=263) = $\\frac{1}{3}\\frac{0.0150}{0.0148} = 33.8\\%$;\n",
    "- P(species=africana | height=263) = $\\frac{1}{3}\\frac{0.0090}{0.0148} = 20.3\\%$.\n",
    "\n",
    "Bayes's Theorem shows us that the largest posterior belongs to the *maximus* species. (Note also that, since the priors are all the same, the largest posterior will necessarily belong to the species with the largest likelihood!)\n",
    "\n",
    "Therefore, the *maximus* species will be our prediction for an elephant of this height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Dimensions\n",
    "\n",
    "In fact, we also have elephant *weight* data available in addition to their heights. To accommodate multiple features we can make use of **multivariate normal** distributions.\n",
    "\n",
    "![multivariate-normal](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8e/MultivariateNormal.png/440px-MultivariateNormal.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elephants = pd.read_csv('data/elephants.csv',\n",
    "                       usecols=['height (in)', 'weight (lbs)', 'species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximus = elephants[elephants['species'] == 'maximus']\n",
    "cyclotis = elephants[elephants['species'] == 'cyclotis']\n",
    "africana = elephants[elephants['species'] == 'africana']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likeli_max = stats.multivariate_normal(mean=maximus.mean(),\n",
    "                          cov=maximus.cov()).pdf([263, 7009])\n",
    "likeli_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likeli_cyc = stats.multivariate_normal(mean=cyclotis.mean(),\n",
    "                         cov=cyclotis.cov()).pdf([263, 7009])\n",
    "likeli_cyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likeli_afr = stats.multivariate_normal(mean=africana.mean(),\n",
    "                         cov=africana.cov()).pdf([263, 7009])\n",
    "likeli_afr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_max = likeli_max / sum([likeli_max, likeli_cyc, likeli_afr])\n",
    "post_cyc = likeli_cyc / sum([likeli_max, likeli_cyc, likeli_afr])\n",
    "post_afr = likeli_afr / sum([likeli_max, likeli_cyc, likeli_afr])\n",
    "\n",
    "print(post_max)\n",
    "print(post_cyc)\n",
    "print(post_afr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GaussianNB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB(priors=[1/3, 1/3, 1/3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = elephants.drop('species', axis=1)\n",
    "y = elephants['species']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.predict_proba(np.array([263, 7009]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(gnb, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comma Survey Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commas = pd.read_csv('data/comma-survey.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first question on the survey was about the Oxford comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commas.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll go ahead and drop the NaNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commas = commas.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commas['In your opinion, which sentence is more gramatically correct?'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personally, I like the Oxford comma, since it can help eliminate ambiguities, such as:\n",
    "\n",
    "\"This book is dedicated to my parents, Ayn Rand, and God\" <br/> vs. <br/>\n",
    "\"This book is dedicated to my parents, Ayn Rand and God\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how a Naive Bayes model would make a prediction here. We'll think of the comma preference as our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commas['Age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to make a prediction about Oxford comma usage for a new person who falls into the **45-60 age group**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Priors and Likelihoods\n",
    "\n",
    "The following code makes a table of values that count up the number of survey respondents who fall into each of eight bins (the four age groups and the two answers to the first comma question). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = np.zeros((2, 4))\n",
    "\n",
    "for idx, value in enumerate(commas['Age'].value_counts().index):\n",
    "    table[0, idx] = len(commas[(commas['In your opinion, which sentence is '\\\n",
    "                                       'more gramatically correct?'] ==\\\n",
    "                                        'It\\'s important for a person to be '\\\n",
    "                                'honest, kind, and loyal.') & (commas['Age'] == value)])\n",
    "    table[1, idx] = len(commas[(commas['In your opinion, which sentence is '\\\n",
    "                                       'more gramatically correct?'] ==\\\n",
    "                                        'It\\'s important for a person to be '\\\n",
    "                                'honest, kind and loyal.') & (commas['Age'] == value)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(table, columns=['Age45-60',\n",
    "                            'Age>60',\n",
    "                            'Age30-44',\n",
    "                            'Age18-29'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Oxford'] = [True, False]\n",
    "df = df[['Age>60', 'Age45-60', 'Age30-44', 'Age18-29', 'Oxford']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all we have is a single categorical feature here we can just read our likelihoods and priors right off of this table:\n",
    "\n",
    "Likelihoods:\n",
    "\n",
    "- Age45-60:\n",
    "    - P(Age45-60 | Oxford=True) = $\\frac{123}{470} = 0.2617$;\n",
    "    - P(Age45-60 | Oxford=False) = $\\frac{125}{355} = 0.3521$.\n",
    "\n",
    "Priors:\n",
    "\n",
    "- P(Oxford=True) = $\\frac{470}{825} = 0.5697$;\n",
    "- P(Oxford=False) = $\\frac{355}{825} = 0.4303$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Posteriors\n",
    "\n",
    "First we'll calculate the probability of the evidence:\n",
    "\n",
    "- P(Age45-60) = P(Age45-60 | Oxford=True) * P(Oxford=True) + P(Age45-60 | Oxford=False) * P(Oxford=False) = 0.2617 * 0.5697 + 0.3521 * 0.4303 = 0.3006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(123+125)/825"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use Bayes's Theorem to calculate the posteriors:\n",
    "\n",
    "- P(Oxford=True | Age45-60) = P(Oxford=True) * P(Age45-60 | Oxford=True) / P(Age45-60) = 0.5697 * 0.2617 / 0.3006 = 0.4960;\n",
    "- P(Oxford=False | Age45-60) = P(Oxford=False) * P(Age45-60 | Oxford=False) / P(Age45-60) = 0.4303 * 0.3521 / 0.3006 = 0.5040.\n",
    "\n",
    "Close! But our prediction for someone in the 45-60 age group will be that they **do not** favor the Oxford comma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with `MultinomialNB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_model = MultinomialNB()\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(commas['Age'].values.reshape(-1, 1))\n",
    "\n",
    "X = ohe.transform(commas['Age'].values.reshape(-1, 1)).todense()\n",
    "y = commas['In your opinion, which sentence is more gramatically correct?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_model.predict_proba(np.array([0, 0, 1, 0]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
